{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:37:15.928628Z",
     "start_time": "2024-12-16T13:37:15.873175Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set() #sets the matplotlib style to seaborn style\n",
    "\n",
    "from scipy.io import loadmat \n",
    "from scipy.ndimage import convolve1d\n",
    "from scipy.signal import butter\n",
    "from scipy.signal import sosfiltfilt\n",
    "from scipy.signal import welch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:37:18.268182Z",
     "start_time": "2024-12-16T13:37:18.209366Z"
    }
   },
   "outputs": [],
   "source": [
    "# load EMG data\n",
    "EMG_file_name='s2/S2_A1_E1.mat'\n",
    "emg_data = loadmat(EMG_file_name)\n",
    "\n",
    "print(f\"What kind of data structure are we working with? {type(emg_data)}\")\n",
    "print(f\"What are the keys of the data structure? {[key for key in emg_data.keys()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:37:43.046610Z",
     "start_time": "2024-12-16T13:37:43.039346Z"
    }
   },
   "outputs": [],
   "source": [
    "emg= emg_data[\"emg\"].copy()\n",
    "stimulus = emg_data[\"restimulus\"] \n",
    "repetition = emg_data[\"rerepetition\"] \n",
    "\n",
    "print(f'What are the shape of the data that we are working with? \\nEMG: {emg.shape}, \\nStimulus {stimulus.shape}, \\nRepetition {repetition.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Apply bandpass filter\n",
    "Before preprocessing, we visualize the rawdata for subject2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:48:29.715909Z",
     "start_time": "2024-12-16T13:48:27.820929Z"
    }
   },
   "outputs": [],
   "source": [
    "n_channels = emg.shape[1]\n",
    "\n",
    "fs = 2000 # According to the references, sampling frequency here is 2000 Hz.\n",
    "\n",
    "time_steps = np.arange(0,emg.shape[0]/fs, 1/fs)\n",
    "\n",
    "fig, ax = plt.subplots(n_channels+2, 1, constrained_layout=True, figsize=(15, 20))\n",
    "\n",
    "ax[0].plot(time_steps, stimulus)\n",
    "ax[0].set_xlabel(\"Time [s]\")\n",
    "ax[0].set_ylabel(\"Stimulus [a.u.]\")\n",
    "\n",
    "ax[1].plot(time_steps, repetition)\n",
    "ax[1].set_xlabel(\"Time [s]\")\n",
    "ax[1].set_ylabel(\"Repetition [a.u.]\")\n",
    "\n",
    "for channel_idx in range(n_channels):\n",
    "    ax[channel_idx+2].plot(time_steps, emg[:, channel_idx]*1000)\n",
    "    ax[channel_idx+2].set_xlabel(\"Time [s]\")\n",
    "    ax[channel_idx+2].set_ylabel(f\"EMG #{channel_idx+1} [uV]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sEMG data was gathered using OttoBock MyoBock 13E200 surface EMG electrodes3, it is supposed to be amplified, bandpass-filtered and rectified version of the raw sEMG signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:16.486303Z",
     "start_time": "2024-12-16T13:52:14.740344Z"
    }
   },
   "outputs": [],
   "source": [
    "bandpass_cutoff_frequencies_Hz = (15, 500) \n",
    "sos = butter(N=0, Wn=bandpass_cutoff_frequencies_Hz, fs=fs, btype=\"bandpass\", output=\"sos\") \n",
    "emg_filtered = sosfiltfilt(sos, emg.T).T \n",
    "\n",
    "\n",
    "powergrid_noise_frequencies_Hz = [harmonic_idx*50 for harmonic_idx in range(1,3)] # removing 50Hz noise and its harmonics\n",
    "\n",
    "for noise_frequency in powergrid_noise_frequencies_Hz:\n",
    "    sos = butter(N=4, Wn=(noise_frequency - 2, noise_frequency + 2), fs=fs, btype=\"bandstop\", output=\"sos\")\n",
    "    emg_filtered = sosfiltfilt(sos, emg_filtered.T).T\n",
    "\n",
    "fig, ax = plt.subplots(n_channels+2, 1, constrained_layout=True, figsize=(15, 15))\n",
    "ax[0].plot(time_steps, stimulus)\n",
    "ax[0].set_xlabel(\"Time [s]\")\n",
    "ax[0].set_ylabel(\"Stimulus [a.u.]\")\n",
    "\n",
    "ax[1].plot(time_steps, repetition)\n",
    "ax[1].set_xlabel(\"Time [s]\")\n",
    "ax[1].set_ylabel(\"Repetition [a.u.]\")\n",
    "for i in range(n_channels):\n",
    "    ax[i+2].plot(time_steps, emg_filtered[:, i])\n",
    "    ax[i+2].set_xlabel(\"Time [s]\")\n",
    "    ax[i+2].set_ylabel(f\"EMG #{channel_idx+1} [uV]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:16.880389Z",
     "start_time": "2024-12-16T13:52:16.499699Z"
    }
   },
   "outputs": [],
   "source": [
    "freqs_pre, Pxx_pre = welch(emg[:, 0], fs=fs, nperseg=1064)\n",
    "freqs_post, Pxx_post = welch(emg_filtered[:, 0],fs = fs,  nperseg=1064)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(10, 3))\n",
    "ax[0].plot(freqs_pre, Pxx_pre, label=\"raw\")\n",
    "ax[0].plot(freqs_post, Pxx_post, linestyle=\"--\", label=\"filtered\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Frequency [Hz]\")\n",
    "ax[0].set_ylabel(\"Power Spectral Density (W/Hz)\")\n",
    "ax[0].set_title(\"Power Spectral Density plot\")\n",
    "\n",
    "ax[1].semilogy(freqs_pre, Pxx_pre, label=\"raw\")\n",
    "ax[1].semilogy(freqs_post, Pxx_post, linestyle=\"--\", label=\"filtered\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Frequency [Hz]\")\n",
    "ax[1].set_ylabel(\"Power Spectral Density (W/Hz)\")\n",
    "ax[1].set_title(\"Power Spectral Density plot in Log scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Rectify signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:19.691337Z",
     "start_time": "2024-12-16T13:52:18.049950Z"
    }
   },
   "outputs": [],
   "source": [
    "emg_rectified = np.abs(emg_filtered)\n",
    "\n",
    "fig, ax = plt.subplots(n_channels, 1, constrained_layout=True, figsize=(15, 15))\n",
    "for channel_idx in range(n_channels):\n",
    "    ax[channel_idx].plot(time_steps, emg_rectified[:, channel_idx])\n",
    "    ax[channel_idx].set_xlabel(\"Time [s]\")\n",
    "    ax[channel_idx].set_ylabel(f\"EMG #{channel_idx+1} [uV]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:32.359057Z",
     "start_time": "2024-12-16T13:52:32.353618Z"
    }
   },
   "outputs": [],
   "source": [
    "# -1 because 0 is the resting condition\n",
    "n_stimuli = len(np.unique(stimulus)) - 1 \n",
    "# -1 because 0 is not a repetition\n",
    "n_repetitions = len(np.unique(repetition)) - 1 \n",
    "n_channels = emg_rectified.shape[1]\n",
    "\n",
    "print(f'How many types of movement are there? {n_stimuli}') \n",
    "print(f'How many repetitions are there? {n_repetitions}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:35.762354Z",
     "start_time": "2024-12-16T13:52:35.747899Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_samples_per_trial = np.zeros((n_stimuli, n_repetitions))\n",
    "\n",
    "for stimuli_idx in range(n_stimuli):\n",
    "    for repetition_idx in range(n_repetitions):\n",
    "        \n",
    "        idx = np.logical_and(stimulus == stimuli_idx+1, repetition == repetition_idx+1)\n",
    "        number_of_samples_per_trial[stimuli_idx, repetition_idx] = np.sum(idx.astype(int))\n",
    "\n",
    "number_of_samples_per_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compute the envelop of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:46.531916Z",
     "start_time": "2024-12-16T13:52:46.467396Z"
    }
   },
   "outputs": [],
   "source": [
    "mov_mean_size = 200\n",
    "mov_mean_weights = np.ones(mov_mean_size) / mov_mean_size\n",
    "\n",
    "#initializing the data structure\n",
    "emg_windows = [[None for repetition_idx in range(n_repetitions)] for stimuli_idx in range(n_stimuli)]\n",
    "emg_envelopes = [[None for repetition_idx in range(n_repetitions)] for stimuli_idx in range(n_stimuli)]\n",
    "\n",
    "for stimuli_idx in range(n_stimuli):\n",
    "    for repetition_idx in range(n_repetitions):\n",
    "        idx = np.logical_and(stimulus == stimuli_idx + 1, repetition == repetition_idx + 1).flatten()\n",
    "        emg_windows[stimuli_idx][repetition_idx] = emg_rectified[idx, :]\n",
    "        emg_envelopes[stimuli_idx][repetition_idx] = convolve1d(emg_windows[stimuli_idx][repetition_idx], mov_mean_weights, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:52:52.272017Z",
     "start_time": "2024-12-16T13:52:50.076998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot all the channels on the same y-scale\n",
    "fig, ax = plt.subplots(2, 5, figsize=(12, 6), constrained_layout=True, sharex=True, sharey=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "number_of_emg_channels = emg_rectified.shape[-1]\n",
    "# plot Rectified EMG signal\n",
    "for channel_idx in range(number_of_emg_channels): \n",
    "    ax[channel_idx].plot(emg_windows[0][0][:, channel_idx])\n",
    "    ax[channel_idx].set_title(f\"Channel {channel_idx+1}\")\n",
    "plt.suptitle(\"Rectified EMG signal\")\n",
    "\n",
    "# plot Envelopes of the EMG signal\n",
    "fig, ax = plt.subplots(2, 5, figsize=(12, 6), constrained_layout=True, sharex=True, sharey=True)\n",
    "ax = ax.ravel()\n",
    "for channel_idx in range(number_of_emg_channels):\n",
    "    ax[channel_idx].plot(emg_envelopes[0][0][:, channel_idx])\n",
    "    ax[channel_idx].set_title(f\"Channel {channel_idx+1}\")\n",
    "plt.suptitle(\"Envelopes of the EMG signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualizing across repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:53:57.872295Z",
     "start_time": "2024-12-16T13:53:53.710214Z"
    }
   },
   "outputs": [],
   "source": [
    "emg_average_activations = np.zeros((n_channels, n_stimuli, n_repetitions))\n",
    "for stimuli_idx in range(n_stimuli):\n",
    "    for repetition_idx in range(n_repetitions):\n",
    "        #mean across time for each channel\n",
    "        emg_average_activations[:, stimuli_idx, repetition_idx] = np.mean(emg_envelopes[stimuli_idx][repetition_idx], axis=0) \n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(10, 6), constrained_layout=True, sharex=True, sharey=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "for stimuli_idx in range(n_stimuli):\n",
    "    sns.heatmap(np.squeeze(emg_average_activations[:, stimuli_idx, :]), ax=ax[stimuli_idx] ,xticklabels=False, yticklabels=False, cbar = True)\n",
    "    ax[stimuli_idx].title.set_text(\"Stimulus \" + str(stimuli_idx + 1))\n",
    "    ax[stimuli_idx].set_xlabel(\"Repetition\")\n",
    "    ax[stimuli_idx].set_ylabel(\"EMG channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = np.arange(n_repetitions)\n",
    "np.random.shuffle(reps)\n",
    "\n",
    "train_reps = reps[:7]  # First 7 repetitions for training\n",
    "val_reps = reps[7:9]   # Next 2 repetitions for validation\n",
    "test_reps = reps[9:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:54:10.312475Z",
     "start_time": "2024-12-16T13:54:10.299325Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_emg = {\n",
    "    'emg' : emg_rectified[np.isin(repetition.flatten(), train_reps),:],\n",
    "    'envelopes' :[emg_envelopes[i] for i in train_reps][0],\n",
    "    'windows' : [emg_envelopes[i] for i in train_reps][0],\n",
    "    'restimulus' : stimulus[np.isin(repetition.flatten(), train_reps),:],\n",
    "    'rerepitions' : repetition[np.isin(repetition.flatten(), train_reps),:]\n",
    "}\n",
    "\n",
    "val_emg = {\n",
    "    'emg' : emg_rectified[np.isin(repetition.flatten(), val_reps),:],\n",
    "    'envelopes' :[emg_envelopes[i] for i in val_reps][0],\n",
    "    'windows' : [emg_envelopes[i] for i in val_reps][0],\n",
    "    'restimulus' : stimulus[np.isin(repetition.flatten(), val_reps),:],\n",
    "    'rerepitions' : repetition[np.isin(repetition.flatten(), val_reps),:]\n",
    "}\n",
    "\n",
    "test_emg = {\n",
    "    'emg' : emg_rectified[np.isin(repetition.flatten(), test_reps),:],\n",
    "    'envelopes' :[emg_envelopes[i] for i in test_reps][0],\n",
    "    'windows' : [emg_envelopes[i] for i in test_reps][0],\n",
    "    'restimulus' : stimulus[np.isin(repetition.flatten(), test_reps),:],\n",
    "    'rerepitions' : repetition[np.isin(repetition.flatten(), test_reps),:]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract features from the trials (at least 5 different ones). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:06:05.753675Z",
     "start_time": "2024-12-16T14:06:05.748300Z"
    }
   },
   "outputs": [],
   "source": [
    "#before\n",
    "mean_std_norm = lambda x: (x - x.mean()) / x.std()\n",
    "def build_dataset_from_ninapro(emg, stimulus, repetition, features=None, feature_norm=False):\n",
    "    # Calculate the number of unique stimuli and repetitions, subtracting 1 to exclude the resting condition\n",
    "    n_stimuli = np.unique(stimulus).size - 1\n",
    "    n_repetitions = np.unique(repetition).size - 1\n",
    "    # Total number of samples is the product of stimuli and repetitions\n",
    "    n_samples = n_stimuli * n_repetitions\n",
    "    \n",
    "    # Number of channels in the EMG data\n",
    "    n_channels = emg.shape[1]\n",
    "    # Calculate the total number of features by summing the number of channels for each feature\n",
    "    n_features = sum(n_channels for feature in features)\n",
    "    \n",
    "    # Initialize the dataset and labels arrays with zeros\n",
    "    dataset = np.zeros((n_samples, n_features))\n",
    "    labels = np.zeros(n_samples)\n",
    "    current_sample_index = 0\n",
    "    \n",
    "    # Loop over each stimulus and repetition to extract features\n",
    "    for i in range(n_stimuli):\n",
    "        for j in range(n_repetitions):\n",
    "            # Assign the label for the current sample\n",
    "            labels[current_sample_index] = i + 1\n",
    "            # Calculate the current sample index based on stimulus and repetition\n",
    "            current_sample_index = i * n_repetitions + j\n",
    "            current_feature_index = 0\n",
    "            # Select the time steps corresponding to the current stimulus and repetition\n",
    "            selected_tsteps = np.logical_and(stimulus == i + 1, repetition == j + 1).squeeze()\n",
    "            \n",
    "            # Loop over each feature function provided\n",
    "            for feature in features:\n",
    "                # Determine the indices in the dataset where the current feature will be stored\n",
    "                selected_features = np.arange(current_feature_index, current_feature_index + n_channels)\n",
    "                # Apply the feature function to the selected EMG data and store the result\n",
    "                cur_feature = feature(emg[selected_tsteps, :])\n",
    "                if feature_norm:\n",
    "                    cur_feature = mean_std_norm(cur_feature)\n",
    "                dataset[current_sample_index, selected_features] = cur_feature\n",
    "                # Update the feature index for the next feature\n",
    "                current_feature_index += n_channels\n",
    "\n",
    "            # Move to the next sample\n",
    "            current_sample_index += 1\n",
    "            \n",
    "    # Return the constructed dataset and corresponding labels\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:12:45.875921Z",
     "start_time": "2024-12-16T14:12:45.586329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the features \n",
    "\n",
    "from scipy.signal import welch\n",
    "psd = lambda x: np.sum(welch(x, axis=0)[1], axis=0)\n",
    "\n",
    "\n",
    "# Mean absolute value (MAV)\n",
    "mav = lambda x: np.mean(np.abs(x), axis=0)\n",
    "# Standard Deviation (STD)\n",
    "std = lambda x: np.std(x, axis=0)\n",
    "# Maximum absolute Value (MaxAV)\n",
    "maxav = lambda x: np.max(np.abs(x), axis=0)\n",
    "# Root mean square (RMS)\n",
    "rms = lambda x: np.sqrt(np.mean(x**2, axis=0))\n",
    "# Waveform length (WL)\n",
    "wl = lambda x: np.sum(np.abs(np.diff(x, axis=0)), axis=0)\n",
    "# Slope sign changes (SSC)\n",
    "ssc = lambda x: np.sum((np.diff(x, axis=0)[:-1, :] * np.diff(x, axis=0)[1:, :]) < 0, axis=0)\n",
    "\n",
    "#Feel free to add more features, e.g. frequency domain features. (See https://doi.org/10.3390/s19204596 and https://doi.org/10.1088/0967-3334/24/2/307)\n",
    "\n",
    "\n",
    "dataset, labels = build_dataset_from_ninapro(\n",
    "    emg=emg_filtered,\n",
    "    stimulus=stimulus,\n",
    "    repetition=repetition,\n",
    "    features=[psd, mav, std, maxav, rms, wl, ssc],\n",
    "    feature_norm=True\n",
    ")\n",
    "\n",
    "print(f\"dataset dimension: {dataset.shape}\")\n",
    "print(f\"labels dimension: {labels.shape}\")\n",
    "sns.heatmap(dataset)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Samples')\n",
    "plt.yticks(range(5, 116, 10), [f'sub%02d' % (x+1) for x in range(12)])\n",
    "plt.xticks(range(5, 56, 10), ['mav', 'std', 'maxav', 'rms', 'wl', 'ssc'])\n",
    "\n",
    "# 获取当前轴\n",
    "ax = plt.gca()\n",
    "\n",
    "# 添加纵向分割线 (x轴)\n",
    "for x in range(10, dataset.shape[1], 10):  # 每隔10个特征\n",
    "    ax.axvline(x=x, color='black', linestyle='-', linewidth=1.5)\n",
    "\n",
    "# 添加横向分割线 (y轴)\n",
    "for y in range(10, dataset.shape[0], 10):  # 每隔10个样本\n",
    "    ax.axhline(y=y, color='black', linestyle='-', linewidth=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform classification on the data of subject 2 - Gradient Boost Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T18:10:03.862306Z",
     "start_time": "2024-12-16T18:10:02.330147Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Train:Validation:Test=70:15:15\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(dataset, labels, test_size=0.3, random_state=42,stratify=labels)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f'n_training: {X_train.shape[0]}, n_validation: {X_val.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "\n",
    "# Normalizing the data\n",
    "# StandardScaler is used to scale the features so that they have a mean of 0 and a standard deviation of 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_val_z = scaler.transform(X_val)\n",
    "\n",
    "# Classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=300, max_depth=3, learning_rate=0.1, min_samples_leaf=4,min_samples_split=2,random_state=42)\n",
    "clf.fit(X_train_z, y_train)\n",
    "\n",
    "# Predict in validation\n",
    "y_pred = clf.predict(X_val_z)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy score: {accuracy}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "confmat = confusion_matrix(y_val, y_pred, normalize=\"true\")\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.heatmap(confmat, annot=True, ax=ax)  # Use seaborn to create a heatmap of the confusion matrix\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "plt.title(\"Confusion Matrix with Validation Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T18:10:45.669925Z",
     "start_time": "2024-12-16T18:10:45.447213Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_z = scaler.transform(X_test)\n",
    "# Predict in test\n",
    "y_pred = clf.predict(X_test_z)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy score: {accuracy}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "confmat = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.heatmap(confmat, annot=True, ax=ax)  # Use seaborn to create a heatmap of the confusion matrix\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "plt.title(\"Confusion Matrix with Testing Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:14:00.644492Z",
     "start_time": "2024-12-16T14:13:54.759278Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, X_train_z, y_train, cv=5)\n",
    "print(f\"Accuracy scores of all models: {scores}\")\n",
    "print(f\"Mean accuracy across all models: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T14:20:33.013212Z",
     "start_time": "2024-12-16T14:14:28.383367Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100,200,300],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Instantiation model\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_z, y_train)\n",
    "\n",
    "# Output optimum parameter\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the validation set with the best parameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_val_z)\n",
    "\n",
    "# Recalculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Test Accuracy after Optimization: {accuracy}\")\n",
    "\n",
    "confmat = confusion_matrix(y_val, y_pred, normalize=\"true\")\n",
    "sns.heatmap(confmat, annot=True)\n",
    "plt.title(\"Confusion Matrix (After Optimization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T18:00:44.384962Z",
     "start_time": "2024-12-16T18:00:44.165030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the test set with the best parameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred = best_clf.predict(X_test_z)\n",
    "\n",
    "# Recalculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy after Optimization:0 {accuracy}\")\n",
    "\n",
    "confmat = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "sns.heatmap(confmat, annot=True)\n",
    "plt.title(\"Confusion Matrix (After Optimization) with Test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalization Across Subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load EMG data of all 27 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:04:51.267803Z",
     "start_time": "2024-12-16T19:04:46.669819Z"
    }
   },
   "outputs": [],
   "source": [
    "emg_all = np.zeros((0, 10))\n",
    "stimulus_all = np.zeros((0, 1))\n",
    "repetition_all = np.zeros((0, 1))\n",
    "\n",
    "for sub_idx in range(1, 27+1):\n",
    "\n",
    "    # 1. Load data\n",
    "\n",
    "    print('---- Loading Data From {sub_idx}')\n",
    "    EMG_file_name = f's{sub_idx}/S{sub_idx}_A1_E1.mat'\n",
    "    emg_data = loadmat(EMG_file_name)\n",
    "    emg = emg_data[\"emg\"].copy()\n",
    "    stimulus = emg_data[\"restimulus\"]\n",
    "    repetition = emg_data[\"rerepetition\"]\n",
    "    print(f'    EMG: {emg.shape}, Stimulus {stimulus.shape}, Repetition {repetition.shape}')\n",
    "\n",
    "    # 2. Perform bandpass filtering\n",
    "\n",
    "    n_channels = emg.shape[1]\n",
    "    fs = 2000\n",
    "    time_steps = np.arange(0,emg.shape[0]/fs, 1/fs)\n",
    "    bandpass_cutoff_frequencies_Hz = (15, 500)\n",
    "    sos = butter(N=0, Wn=bandpass_cutoff_frequencies_Hz, fs=fs, btype=\"bandpass\", output=\"sos\")\n",
    "    emg_filtered = sosfiltfilt(sos, emg.T).T\n",
    "    powergrid_noise_frequencies_Hz = [harmonic_idx*50 for harmonic_idx in range(1,3)] # removing 50Hz noise and its harmonics\n",
    "    for noise_frequency in powergrid_noise_frequencies_Hz:\n",
    "        sos = butter(N=4, Wn=(noise_frequency - 2, noise_frequency + 2), fs=fs, btype=\"bandstop\", output=\"sos\")\n",
    "        emg_filtered = sosfiltfilt(sos, emg_filtered.T).T\n",
    "\n",
    "    # 3. Rectify\n",
    "    emg_rectified = np.abs(emg_filtered)\n",
    "    n_stimuli = len(np.unique(stimulus)) - 1\n",
    "    n_repetitions = len(np.unique(repetition)) - 1\n",
    "    n_channels = emg_rectified.shape[1]\n",
    "    print(f'    n_stimuli: {n_stimuli}, n_repetitions: {n_repetitions}, n_channels: {n_channels}\\n')\n",
    "\n",
    "    # 4. Moving average\n",
    "    mov_mean_size = 200\n",
    "    mov_mean_weights = np.ones(mov_mean_size) / mov_mean_size\n",
    "\n",
    "    #initializing the data structure\n",
    "    emg_windows = [[None for repetition_idx in range(n_repetitions)] for stimuli_idx in range(n_stimuli)]\n",
    "    emg_envelopes = [[None for repetition_idx in range(n_repetitions)] for stimuli_idx in range(n_stimuli)]\n",
    "\n",
    "    for stimuli_idx in range(n_stimuli):\n",
    "        for repetition_idx in range(n_repetitions):\n",
    "            idx = np.logical_and(stimulus == stimuli_idx + 1, repetition == repetition_idx + 1).flatten()\n",
    "            emg_windows[stimuli_idx][repetition_idx] = emg_rectified[idx, :]\n",
    "            emg_envelopes[stimuli_idx][repetition_idx] = convolve1d(emg_windows[stimuli_idx][repetition_idx], mov_mean_weights, axis=0)\n",
    "\n",
    "    emg_all = np.vstack((emg_all, emg))\n",
    "    stimulus_all = np.vstack((stimulus_all, stimulus))\n",
    "    repetition_all = np.vstack((repetition_all, repetition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:05:01.138797Z",
     "start_time": "2024-12-16T19:04:59.195425Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset, labels = build_dataset_from_ninapro(\n",
    "    emg=emg_all,\n",
    "    stimulus=stimulus_all,\n",
    "    repetition=repetition_all,\n",
    "    features=[mav, std, maxav, rms, wl, ssc],\n",
    "    feature_norm=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSSP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
